\documentclass[lang=cn,11pt,a4paper,cite=authoryear]{elegantpaper}

\title{COVID-19疫情趋势预测研究}
\author{肖世莺 \and 张红丽 \and 成宏媛 \and 王瑜}
\date{}

\usepackage{array}
\newcommand{\ccr}[1]{\makecell{{\color{#1}\rule{1cm}{1cm}}}}
\usepackage{subfigure}

\begin{document}

\maketitle

\begin{abstract}
本文。。。。。。。
如果想要了解本文的相关数据和程序代码，请访问
\href{https://github.com/data-science-in-action/project-stat223}{Github::project-stat223}
。
\keywords{COVID-19，SEIR，LSTM，预测分析}
\end{abstract}

\section{引言}
COVID-19已构成全球性大流行，并已蔓延到世界上大多数国家和地区。通过了解某个地区确诊病例
的发展趋势，政府可以采取相应的政策以控制应对疫情。但是，单一的模型估计可能会得出有偏的结
果，不同数学模型产生的预测结果是不一致的。。。。。。。

\section{相关研究}

\subsection{人口增长预测模型}

\subsection{传染病模型}

\subsection{机器学习的数量预测应用}

\subsection{COVID-19的预测研究}

\section{研究方法}

\subsection{SEIR模型}
SEIR模型将研究对象分为S、E、I、R4种类型；1）$S$为易感状态（susceptible），表示潜在的可
感染人群，个体在感染之前是处于易感状态的，即该个体有可能被邻居个体感染。2）$E$为潜伏状态
（exposed），表示已经被感染但没有表现出感染症状来的群体。3）$I$为感染状态（infected），
表示表现出感染症状的人，该个体还会以一定的概率感染其能接触到的易感个体。4）$R$为移出状态
（removed），表示脱离系统不再受到传染病影响的人。

记$S(t)$、$E(t)$、$I(t)$、$R(t)$分别为时刻$t$的易感人群数、潜伏人群数、感染人群数、移出
人群数，显然有$S(t)+E(t)+I(t)+R(t)=N$，其中$N$为种群的个体数。

假设一个易感状态在单位时间$\tau$里与感染个体接触并被传染的概率为$\beta$。由于易感个体的
比例为$S/N$，时刻$t$网络中总共有$I(t)$个感染个体，所以易感个体的数目按照如下变化率变化：
\begin{equation}
\frac{dS}{dt}=-\frac{\beta SI}{N}
\end{equation}
相应地，潜伏个体的数目按照如下变化率增加，并且整体以单位时间概率$\sigma$转化为感染个体：
\begin{equation}
\frac{dE}{dt}=\frac{\beta SI}{N}-\sigma E
\end{equation}
感染个体数目由潜伏群体提供，个体同时以单位时间概率$\gamma$转化为移除状态：
\begin{equation}
\frac{dI}{dt}=\sigma E-\gamma I
\end{equation}
相应地移除个体以概率$\gamma$由感染群体往移除个体转化：
\begin{equation}
\frac{dR}{dt}=\gamma I
\end{equation}
\subsection{LSTM模型}
LSTM模型（Long Short-Term Memory）是一种用于深度学习领域的递归神经网络（RNN）架构在传统
的RNN模型中，所使用的训练算法是时序反向传播算法（Back Propagation Trough 
Time，即BPTT）。循环网络的目的是能够对输入的序列进行准确的分类，这主要靠误差值（预测结果
与真实结果的差距）的反向传播和梯度下降来实现。反向传播从最后的误差值开始，将误差值以某种
形式，经过每个隐藏层向输入层逐层反向传播，按照一定的比例将误差分摊给所有的单元，从而获得
各层单元的误差信号，以此作为修正各个单元权值的依据。反向传播算法的目的就是寻找能够最大限
度地减小误差的权重。最小化误差的实质是一个优化问题，这就需要计算误差对权重的偏导，将偏导
数运用到梯度下降算法中，以调整权重减少误差。而BPTT是循环网络依赖于反向传播的一种扩展，通
过一系列定义明确、有序的计算来表达时间，这些计算将一个时间步与下一个时间步联系起来。当模
型运算时间较长时，需要回传的残差会呈现指数下降，导致网络权重更新缓慢，使RNN模型的长期记
忆效果无法得以体现，因此LSTM模型应运而生。LSTM模型由\cite{hochreiter1997lstm}提出，在
各隐藏层的神经元单元之间增加了记忆单元，可以学习长期依赖信息，避免了RNN无法解决的长期依
赖问题。此后许多学者对其进行了改进，使得LSTM在许多问题上得到了广泛的使用，并取得相当巨大
的成功\citep{gers2000learning, graves2005bidirectional, graves2005framewise, 
schmidhuber2007training, bayer2009evolving, schaul2010pybrain, 
graves2013hybrid, bayer2014Learning}。

\begin{figure}[htp]
	\centering
	\subfigure[RNN] {
		\begin{minipage}{0.48\linewidth}
			\centering
			\includegraphics[width=7.0cm]{RNN.pdf}
		\end{minipage}
	}
	\subfigure[LSTM] {
		\begin{minipage}{0.48\linewidth}
			\centering
			\includegraphics[width=7.0cm]{LSTM.pdf}
		\end{minipage}
	}
    \caption{RNN模型和LSTM模型的结构}
	\label{fig:structure}
\end{figure}

图\ref{fig:structure}显示了RNN模型与LSTM模型的结构，其中蓝色圆圈$x_t$表示输入信息，紫
色圆圈$h_t$表示输出信息，黄色框表示神经网络层，粉色圆圈表示逐点运算，例如矢量加法。每条
线上都承载着从一个节点的输出到另一个节点的输入的矢量。合并的线表示向量的连接，而分开的线
表示其内容被复制，然后分配到不同的位置。由图\ref{fig:structure}可见，RNN与LSTM最大的
区别在于——LSTM中最顶层多了一条信息传送带，即细胞状态$C_t$，也就是信息记忆的地方，这也是LSTM
的核心。

LSTM通过称作门（gate）的结构调节，具有删除或添加信息到细胞状态的能力。门是一种选择性地让
信息通过的方式，由$\sigma$（sigmoid）神经网络层和逐点乘法运算组成。$\sigma$层输出$0$
到$1$之间的数值，描述每个信息量可以通过多少。$0$代表“不许任何量通过”，$1$代表“允许任意
量通过”。常见的LSTM单元由一个细胞、一个输入门（input gate）、一个输出门（output gate）
和一个遗忘门（forget gate）组成。细胞会记住任意时间间隔内的值，并且三个门控制着进出单元
的信息流。

LSTM模型中，第一步是决定从细胞状态中丢弃哪些信息。这个决定通过一个称为“遗忘门层”的$\sigma$
层决定。该门会读取$h_{t-1}$和$x_t$作为输入，为细胞状态$C_t$中的每个数输出一个在$0$到$1$
之间的数值，记为$f_t$，表示保留多少信息，$1$表示“完全保留信息”，$0$表示“完全舍弃信息”。
\begin{equation}
f_t=\sigma(W_{f} \cdot [h_{t-1},x_{t}]+b_{f})
\end{equation}

第二步是在细胞状态中存储哪些信息。首先是由称为“输入门层”的$\sigma$层决定哪些信息需要更
新，该概率表示为$i_t$；然后输入门层中的tanh层创建一个新的候选值向量$\tilde{C_t}$，将其
增加到细胞状态中。
\begin{gather}
i_t=\sigma(W_{i} \cdot [h_{t-1},x_{t}]+b_i) \\
\tilde{C_t}={\rm tanh}(W_{C} \cdot [h_{t-1},x_{t}]+b_C)
\end{gather}

第三步是更新旧的细胞状态。$f_t$表示忘记上一次信息$C_t$的程度，$i_t$表示将候选值$\tilde{C_t}$
加入的程度，通过对第二步中两个信息的结合，真正实现了移除哪些旧的信息，增加哪些新信息，最
后得到了本细胞的状态$C_t$。
\begin{equation}
C_t=f_t*C_{t-1}+i_t*\tilde{C_t}
\end{equation}

最后是确定要输出的内容，即决定作出什么样的预测。首先，通过运行称为“输出门层”的$\sigma$
层来决定输出细胞状态$C_t$的哪些部分；然后，将细胞状态通过tanh层进行处理，使值在$-1$与$1$
之间，然后与$\sigma$层的输出相乘，最终输出决定输出的那部分。
\begin{gather}
o_t=\sigma(W_o \cdot [h_{t-1},x_t]+b_o) \\
h_t=o_t*{\rm tanh}(C_t)
\end{gather}

\begin{figure}[htp]
	\centering
	\subfigure[遗忘门：舍弃信息] {
		\begin{minipage}{0.48\linewidth}
			\centering
			\includegraphics[width=7.0cm]{forget.pdf}
		\end{minipage}
	}
    \subfigure[输入门：存储信息] {
		\begin{minipage}{0.48\linewidth}
			\centering
			\includegraphics[width=7.0cm]{input.pdf}
		\end{minipage}
	}
	\\
	\subfigure[更新细胞状态] {
		\begin{minipage}{0.48\linewidth}
			\centering
			\includegraphics[width=7.0cm]{update.pdf}
		\end{minipage}
	}
	\subfigure[输出门：输出] {
		\begin{minipage}{0.48\linewidth}
			\centering
			\includegraphics[width=7.0cm]{output.pdf}
		\end{minipage}
	}
    \caption{LSTM信息的流动}
	\label{fig:LSTM}
\end{figure}

\section{实证分析}

\subsection{SEIR模型}

\subsection{LSTM模型}

\section{讨论}

\bibliography{report}

\end{document}
